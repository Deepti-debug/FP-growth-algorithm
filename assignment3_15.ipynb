{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816bfbf3",
   "metadata": {},
   "source": [
    "#### **1. Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97256b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                 # Importing for panel data analysis\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt                                     # Importing pyplot interface of matplotlib\n",
    "import seaborn as sns                                               # Importing seaborn library for interactive visualization\n",
    "%matplotlib inline\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import warnings                                                     # Importing warning to disable runtime warnings\n",
    "warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9f7b95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (100836, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "print('Shape of the dataset:', ratings.shape)\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "55e3084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8fc58e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique users:\", ratings[\"userId\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b361bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies: 9724\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique movies:\", ratings[\"movieId\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe62d6",
   "metadata": {},
   "source": [
    "#### **2. Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87862091",
   "metadata": {},
   "source": [
    "#### Form the transactional data set, which consists of entries of the form <user id, {movies rated above 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0ef04632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId\n",
       "1             [4.0, 4.5, 2.5, 3.5, 3.0, 5.0, 0.5, 2.0, 1.5]\n",
       "2         [4.0, 3.0, 3.5, 4.5, 2.5, 5.0, 1.5, 1.0, 2.0, ...\n",
       "3             [4.0, 5.0, 3.0, 3.5, 2.0, 1.0, 2.5, 0.5, 1.5]\n",
       "4                                      [3.0, 1.0, 2.0, 1.5]\n",
       "5             [5.0, 3.0, 4.0, 2.0, 3.5, 4.5, 1.5, 2.5, 0.5]\n",
       "                                ...                        \n",
       "193581                                                [4.0]\n",
       "193583                                                [3.5]\n",
       "193585                                                [3.5]\n",
       "193587                                                [3.5]\n",
       "193609                                                [4.0]\n",
       "Length: 9724, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's extract the number of unique movies and its corresponding ratings\n",
    "group = ratings.groupby('movieId')\n",
    "df = group.apply(lambda x: x['rating'].unique())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d1154",
   "metadata": {},
   "source": [
    "- So, there are movies that have been rated 2 or less. Let's keep only entries where movie ratings are greater than 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2bd716b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87313 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[87313 rows x 4 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_above_2 = ratings[ratings[\"rating\"] > 2.0]\n",
    "ratings_above_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2ad4bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId\n",
       "1       226\n",
       "2        28\n",
       "3        18\n",
       "4       167\n",
       "5        40\n",
       "       ... \n",
       "606    1070\n",
       "607     174\n",
       "608     670\n",
       "609      37\n",
       "610    1233\n",
       "Length: 610, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's extract the number of unique movies that each user might have rated\n",
    "group = ratings_above_2.groupby('userId')\n",
    "df = group.apply(lambda x: len(x['movieId'].unique()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d3631beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 226,\n",
       " 2: 28,\n",
       " 3: 18,\n",
       " 4: 167,\n",
       " 5: 40,\n",
       " 6: 294,\n",
       " 7: 111,\n",
       " 8: 43,\n",
       " 9: 34,\n",
       " 10: 119,\n",
       " 11: 59,\n",
       " 12: 32,\n",
       " 13: 28,\n",
       " 14: 42,\n",
       " 15: 111,\n",
       " 16: 96,\n",
       " 17: 105,\n",
       " 18: 493,\n",
       " 19: 357,\n",
       " 20: 210,\n",
       " 21: 380,\n",
       " 22: 70,\n",
       " 23: 120,\n",
       " 24: 107,\n",
       " 25: 26,\n",
       " 26: 19,\n",
       " 27: 109,\n",
       " 28: 476,\n",
       " 29: 78,\n",
       " 30: 34,\n",
       " 31: 45,\n",
       " 32: 99,\n",
       " 33: 137,\n",
       " 34: 67,\n",
       " 35: 22,\n",
       " 36: 35,\n",
       " 37: 20,\n",
       " 38: 63,\n",
       " 39: 90,\n",
       " 40: 94,\n",
       " 41: 170,\n",
       " 42: 353,\n",
       " 43: 114,\n",
       " 44: 38,\n",
       " 45: 366,\n",
       " 46: 42,\n",
       " 47: 111,\n",
       " 48: 33,\n",
       " 49: 21,\n",
       " 50: 236,\n",
       " 51: 319,\n",
       " 52: 130,\n",
       " 53: 20,\n",
       " 54: 31,\n",
       " 55: 16,\n",
       " 56: 46,\n",
       " 57: 379,\n",
       " 58: 103,\n",
       " 59: 101,\n",
       " 60: 22,\n",
       " 61: 37,\n",
       " 62: 357,\n",
       " 63: 248,\n",
       " 64: 504,\n",
       " 65: 34,\n",
       " 66: 337,\n",
       " 67: 33,\n",
       " 68: 1085,\n",
       " 69: 44,\n",
       " 70: 61,\n",
       " 71: 30,\n",
       " 72: 45,\n",
       " 73: 187,\n",
       " 74: 177,\n",
       " 75: 51,\n",
       " 76: 87,\n",
       " 77: 25,\n",
       " 78: 47,\n",
       " 79: 60,\n",
       " 80: 167,\n",
       " 81: 17,\n",
       " 82: 207,\n",
       " 83: 95,\n",
       " 84: 287,\n",
       " 85: 27,\n",
       " 86: 69,\n",
       " 87: 20,\n",
       " 88: 52,\n",
       " 89: 425,\n",
       " 90: 53,\n",
       " 91: 495,\n",
       " 92: 24,\n",
       " 93: 97,\n",
       " 94: 44,\n",
       " 95: 160,\n",
       " 96: 66,\n",
       " 97: 35,\n",
       " 98: 81,\n",
       " 99: 44,\n",
       " 100: 141,\n",
       " 101: 50,\n",
       " 102: 52,\n",
       " 103: 362,\n",
       " 104: 254,\n",
       " 105: 717,\n",
       " 106: 33,\n",
       " 107: 34,\n",
       " 108: 69,\n",
       " 109: 118,\n",
       " 110: 47,\n",
       " 111: 557,\n",
       " 112: 51,\n",
       " 113: 123,\n",
       " 114: 28,\n",
       " 115: 94,\n",
       " 116: 74,\n",
       " 117: 157,\n",
       " 118: 20,\n",
       " 119: 214,\n",
       " 120: 19,\n",
       " 121: 52,\n",
       " 122: 291,\n",
       " 123: 56,\n",
       " 124: 48,\n",
       " 125: 343,\n",
       " 126: 29,\n",
       " 127: 14,\n",
       " 128: 33,\n",
       " 129: 138,\n",
       " 130: 25,\n",
       " 131: 65,\n",
       " 132: 277,\n",
       " 133: 29,\n",
       " 134: 34,\n",
       " 135: 238,\n",
       " 136: 88,\n",
       " 137: 139,\n",
       " 138: 17,\n",
       " 139: 85,\n",
       " 140: 569,\n",
       " 141: 164,\n",
       " 142: 37,\n",
       " 143: 52,\n",
       " 144: 125,\n",
       " 145: 21,\n",
       " 146: 26,\n",
       " 147: 15,\n",
       " 148: 47,\n",
       " 149: 32,\n",
       " 150: 26,\n",
       " 151: 54,\n",
       " 152: 62,\n",
       " 153: 88,\n",
       " 154: 32,\n",
       " 155: 44,\n",
       " 156: 369,\n",
       " 157: 17,\n",
       " 158: 26,\n",
       " 159: 76,\n",
       " 160: 222,\n",
       " 161: 36,\n",
       " 162: 38,\n",
       " 163: 18,\n",
       " 164: 36,\n",
       " 165: 62,\n",
       " 166: 189,\n",
       " 167: 148,\n",
       " 168: 90,\n",
       " 169: 269,\n",
       " 170: 47,\n",
       " 171: 82,\n",
       " 172: 26,\n",
       " 173: 24,\n",
       " 174: 62,\n",
       " 175: 19,\n",
       " 176: 35,\n",
       " 177: 795,\n",
       " 178: 73,\n",
       " 179: 69,\n",
       " 180: 22,\n",
       " 181: 86,\n",
       " 182: 861,\n",
       " 183: 53,\n",
       " 184: 118,\n",
       " 185: 41,\n",
       " 186: 225,\n",
       " 187: 225,\n",
       " 188: 47,\n",
       " 189: 20,\n",
       " 190: 66,\n",
       " 191: 68,\n",
       " 192: 22,\n",
       " 193: 31,\n",
       " 194: 19,\n",
       " 195: 162,\n",
       " 196: 30,\n",
       " 197: 31,\n",
       " 198: 184,\n",
       " 199: 299,\n",
       " 200: 319,\n",
       " 201: 102,\n",
       " 202: 389,\n",
       " 203: 42,\n",
       " 204: 75,\n",
       " 205: 27,\n",
       " 206: 25,\n",
       " 207: 14,\n",
       " 208: 19,\n",
       " 209: 32,\n",
       " 210: 129,\n",
       " 211: 83,\n",
       " 212: 244,\n",
       " 213: 84,\n",
       " 214: 18,\n",
       " 215: 94,\n",
       " 216: 140,\n",
       " 217: 392,\n",
       " 218: 20,\n",
       " 219: 427,\n",
       " 220: 188,\n",
       " 221: 323,\n",
       " 222: 215,\n",
       " 223: 65,\n",
       " 224: 46,\n",
       " 225: 67,\n",
       " 226: 459,\n",
       " 227: 94,\n",
       " 228: 23,\n",
       " 229: 65,\n",
       " 230: 98,\n",
       " 231: 21,\n",
       " 232: 786,\n",
       " 233: 136,\n",
       " 234: 166,\n",
       " 235: 59,\n",
       " 236: 24,\n",
       " 237: 47,\n",
       " 238: 40,\n",
       " 239: 274,\n",
       " 240: 125,\n",
       " 241: 75,\n",
       " 242: 34,\n",
       " 243: 36,\n",
       " 244: 77,\n",
       " 245: 15,\n",
       " 246: 199,\n",
       " 247: 137,\n",
       " 248: 50,\n",
       " 249: 1027,\n",
       " 250: 26,\n",
       " 251: 23,\n",
       " 252: 38,\n",
       " 253: 46,\n",
       " 254: 120,\n",
       " 255: 19,\n",
       " 256: 170,\n",
       " 257: 16,\n",
       " 258: 23,\n",
       " 259: 20,\n",
       " 260: 141,\n",
       " 261: 49,\n",
       " 262: 45,\n",
       " 263: 196,\n",
       " 264: 50,\n",
       " 265: 136,\n",
       " 266: 134,\n",
       " 267: 50,\n",
       " 268: 87,\n",
       " 269: 27,\n",
       " 270: 30,\n",
       " 271: 40,\n",
       " 272: 29,\n",
       " 273: 52,\n",
       " 274: 1174,\n",
       " 275: 362,\n",
       " 276: 41,\n",
       " 277: 27,\n",
       " 278: 19,\n",
       " 279: 163,\n",
       " 280: 191,\n",
       " 281: 18,\n",
       " 282: 230,\n",
       " 283: 28,\n",
       " 284: 83,\n",
       " 285: 34,\n",
       " 286: 101,\n",
       " 287: 89,\n",
       " 288: 854,\n",
       " 289: 23,\n",
       " 290: 261,\n",
       " 291: 30,\n",
       " 292: 371,\n",
       " 293: 10,\n",
       " 294: 233,\n",
       " 295: 33,\n",
       " 296: 23,\n",
       " 297: 38,\n",
       " 298: 574,\n",
       " 299: 21,\n",
       " 300: 32,\n",
       " 301: 93,\n",
       " 302: 31,\n",
       " 303: 49,\n",
       " 304: 194,\n",
       " 305: 653,\n",
       " 306: 101,\n",
       " 307: 619,\n",
       " 308: 59,\n",
       " 309: 98,\n",
       " 310: 52,\n",
       " 311: 17,\n",
       " 312: 208,\n",
       " 313: 262,\n",
       " 314: 103,\n",
       " 315: 29,\n",
       " 316: 46,\n",
       " 317: 166,\n",
       " 318: 878,\n",
       " 319: 35,\n",
       " 320: 19,\n",
       " 321: 56,\n",
       " 322: 96,\n",
       " 323: 87,\n",
       " 324: 15,\n",
       " 325: 232,\n",
       " 326: 142,\n",
       " 327: 45,\n",
       " 328: 195,\n",
       " 329: 11,\n",
       " 330: 224,\n",
       " 331: 147,\n",
       " 332: 250,\n",
       " 333: 14,\n",
       " 334: 147,\n",
       " 335: 25,\n",
       " 336: 56,\n",
       " 337: 77,\n",
       " 338: 25,\n",
       " 339: 367,\n",
       " 340: 28,\n",
       " 341: 51,\n",
       " 342: 48,\n",
       " 343: 57,\n",
       " 344: 57,\n",
       " 345: 62,\n",
       " 346: 160,\n",
       " 347: 45,\n",
       " 348: 55,\n",
       " 349: 36,\n",
       " 350: 38,\n",
       " 351: 137,\n",
       " 352: 275,\n",
       " 353: 75,\n",
       " 354: 221,\n",
       " 355: 26,\n",
       " 356: 284,\n",
       " 357: 377,\n",
       " 358: 30,\n",
       " 359: 68,\n",
       " 360: 15,\n",
       " 361: 77,\n",
       " 362: 108,\n",
       " 363: 25,\n",
       " 364: 21,\n",
       " 365: 187,\n",
       " 366: 29,\n",
       " 367: 162,\n",
       " 368: 324,\n",
       " 369: 124,\n",
       " 370: 84,\n",
       " 371: 41,\n",
       " 372: 180,\n",
       " 373: 72,\n",
       " 374: 32,\n",
       " 375: 30,\n",
       " 376: 126,\n",
       " 377: 124,\n",
       " 378: 47,\n",
       " 379: 29,\n",
       " 380: 1106,\n",
       " 381: 431,\n",
       " 382: 249,\n",
       " 383: 32,\n",
       " 384: 49,\n",
       " 385: 180,\n",
       " 386: 45,\n",
       " 387: 897,\n",
       " 388: 25,\n",
       " 389: 33,\n",
       " 390: 74,\n",
       " 391: 315,\n",
       " 392: 17,\n",
       " 393: 100,\n",
       " 394: 15,\n",
       " 395: 52,\n",
       " 396: 19,\n",
       " 397: 22,\n",
       " 398: 43,\n",
       " 399: 33,\n",
       " 400: 43,\n",
       " 401: 69,\n",
       " 402: 48,\n",
       " 403: 32,\n",
       " 404: 60,\n",
       " 405: 120,\n",
       " 406: 15,\n",
       " 407: 22,\n",
       " 408: 138,\n",
       " 409: 108,\n",
       " 410: 158,\n",
       " 411: 105,\n",
       " 412: 90,\n",
       " 413: 51,\n",
       " 414: 2239,\n",
       " 415: 93,\n",
       " 416: 36,\n",
       " 417: 67,\n",
       " 418: 77,\n",
       " 419: 140,\n",
       " 420: 140,\n",
       " 421: 35,\n",
       " 422: 72,\n",
       " 423: 18,\n",
       " 424: 129,\n",
       " 425: 306,\n",
       " 426: 75,\n",
       " 427: 65,\n",
       " 428: 182,\n",
       " 429: 56,\n",
       " 430: 50,\n",
       " 431: 13,\n",
       " 432: 242,\n",
       " 433: 20,\n",
       " 434: 212,\n",
       " 435: 42,\n",
       " 436: 96,\n",
       " 437: 112,\n",
       " 438: 570,\n",
       " 439: 20,\n",
       " 440: 33,\n",
       " 441: 44,\n",
       " 442: 2,\n",
       " 443: 36,\n",
       " 444: 40,\n",
       " 445: 70,\n",
       " 446: 73,\n",
       " 447: 76,\n",
       " 448: 1255,\n",
       " 449: 32,\n",
       " 450: 48,\n",
       " 451: 33,\n",
       " 452: 200,\n",
       " 453: 285,\n",
       " 454: 44,\n",
       " 455: 55,\n",
       " 456: 41,\n",
       " 457: 38,\n",
       " 458: 57,\n",
       " 459: 26,\n",
       " 460: 81,\n",
       " 461: 18,\n",
       " 462: 378,\n",
       " 463: 32,\n",
       " 464: 121,\n",
       " 465: 112,\n",
       " 466: 110,\n",
       " 467: 15,\n",
       " 468: 27,\n",
       " 469: 414,\n",
       " 470: 80,\n",
       " 471: 27,\n",
       " 472: 28,\n",
       " 473: 30,\n",
       " 474: 1853,\n",
       " 475: 155,\n",
       " 476: 68,\n",
       " 477: 565,\n",
       " 478: 14,\n",
       " 479: 136,\n",
       " 480: 708,\n",
       " 481: 18,\n",
       " 482: 104,\n",
       " 483: 657,\n",
       " 484: 257,\n",
       " 485: 20,\n",
       " 486: 53,\n",
       " 487: 47,\n",
       " 488: 104,\n",
       " 489: 494,\n",
       " 490: 96,\n",
       " 491: 61,\n",
       " 492: 120,\n",
       " 493: 52,\n",
       " 494: 20,\n",
       " 495: 241,\n",
       " 496: 26,\n",
       " 497: 41,\n",
       " 498: 33,\n",
       " 499: 25,\n",
       " 500: 65,\n",
       " 501: 39,\n",
       " 502: 26,\n",
       " 503: 81,\n",
       " 504: 85,\n",
       " 505: 30,\n",
       " 506: 36,\n",
       " 507: 21,\n",
       " 508: 6,\n",
       " 509: 427,\n",
       " 510: 83,\n",
       " 511: 52,\n",
       " 512: 48,\n",
       " 513: 31,\n",
       " 514: 320,\n",
       " 515: 26,\n",
       " 516: 25,\n",
       " 517: 209,\n",
       " 518: 21,\n",
       " 519: 26,\n",
       " 520: 176,\n",
       " 521: 40,\n",
       " 522: 182,\n",
       " 523: 75,\n",
       " 524: 111,\n",
       " 525: 470,\n",
       " 526: 58,\n",
       " 527: 158,\n",
       " 528: 59,\n",
       " 529: 20,\n",
       " 530: 26,\n",
       " 531: 19,\n",
       " 532: 49,\n",
       " 533: 38,\n",
       " 534: 495,\n",
       " 535: 16,\n",
       " 536: 31,\n",
       " 537: 43,\n",
       " 538: 37,\n",
       " 539: 35,\n",
       " 540: 40,\n",
       " 541: 75,\n",
       " 542: 94,\n",
       " 543: 68,\n",
       " 544: 21,\n",
       " 545: 21,\n",
       " 546: 48,\n",
       " 547: 20,\n",
       " 548: 24,\n",
       " 549: 16,\n",
       " 550: 27,\n",
       " 551: 111,\n",
       " 552: 147,\n",
       " 553: 83,\n",
       " 554: 75,\n",
       " 555: 486,\n",
       " 556: 32,\n",
       " 557: 25,\n",
       " 558: 51,\n",
       " 559: 123,\n",
       " 560: 450,\n",
       " 561: 450,\n",
       " 562: 230,\n",
       " 563: 196,\n",
       " 564: 145,\n",
       " 565: 27,\n",
       " 566: 63,\n",
       " 567: 189,\n",
       " 568: 22,\n",
       " 569: 20,\n",
       " 570: 184,\n",
       " 571: 51,\n",
       " 572: 142,\n",
       " 573: 282,\n",
       " 574: 21,\n",
       " 575: 28,\n",
       " 576: 14,\n",
       " 577: 148,\n",
       " 578: 25,\n",
       " 579: 69,\n",
       " 580: 390,\n",
       " 581: 40,\n",
       " 582: 56,\n",
       " 583: 50,\n",
       " 584: 77,\n",
       " 585: 61,\n",
       " 586: 208,\n",
       " 587: 156,\n",
       " 588: 45,\n",
       " 589: 38,\n",
       " 590: 662,\n",
       " 591: 39,\n",
       " 592: 87,\n",
       " 593: 81,\n",
       " 594: 202,\n",
       " 595: 19,\n",
       " 596: 392,\n",
       " 597: 398,\n",
       " 598: 16,\n",
       " 599: 1794,\n",
       " 600: 582,\n",
       " 601: 101,\n",
       " 602: 118,\n",
       " 603: 774,\n",
       " 604: 96,\n",
       " 605: 195,\n",
       " 606: 1070,\n",
       " 607: 174,\n",
       " 608: 670,\n",
       " 609: 37,\n",
       " 610: 1233}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_freq = dict(df)\n",
    "count_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4cf8e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>count_freq_userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87313 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp  count_freq_userId\n",
       "0            1        1     4.0   964982703                226\n",
       "1            1        3     4.0   964981247                226\n",
       "2            1        6     4.0   964982224                226\n",
       "3            1       47     5.0   964983815                226\n",
       "4            1       50     5.0   964982931                226\n",
       "...        ...      ...     ...         ...                ...\n",
       "100831     610   166534     4.0  1493848402               1233\n",
       "100832     610   168248     5.0  1493850091               1233\n",
       "100833     610   168250     5.0  1494273047               1233\n",
       "100834     610   168252     5.0  1493846352               1233\n",
       "100835     610   170875     3.0  1493846415               1233\n",
       "\n",
       "[87313 rows x 5 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column to record the number of movies rated by each userId\n",
    "ratings_above_2['count_freq_userId'] = ratings_above_2['userId']\n",
    "ratings_above_2['count_freq_userId'] = ratings_above_2['count_freq_userId'].map(count_freq)\n",
    "ratings_above_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69067934",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975fc84",
   "metadata": {},
   "source": [
    "- Let's keep only those users who have rated more than 10 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9460a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  movieId  rating   timestamp  count_freq_userId\n",
      "0            1        1     4.0   964982703                226\n",
      "1            1        3     4.0   964981247                226\n",
      "2            1        6     4.0   964982224                226\n",
      "3            1       47     5.0   964983815                226\n",
      "4            1       50     5.0   964982931                226\n",
      "...        ...      ...     ...         ...                ...\n",
      "100831     610   166534     4.0  1493848402               1233\n",
      "100832     610   168248     5.0  1493850091               1233\n",
      "100833     610   168250     5.0  1494273047               1233\n",
      "100834     610   168252     5.0  1493846352               1233\n",
      "100835     610   170875     3.0  1493846415               1233\n",
      "\n",
      "[87295 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now let's remove the rows where the value of 'count_freq_userId' is less than 10.\n",
    "more_than_10_movies_rated_above_2 = ratings_above_2.drop(ratings_above_2[ratings_above_2['count_freq_userId'] <= 10].index)\n",
    "print(more_than_10_movies_rated_above_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd84e5f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f229d5",
   "metadata": {},
   "source": [
    "- Let's create the transactional data of the form <user id, {movies rated above 2}>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9266f5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{1024, 1, 1025, 3, 2048, 1029, 6, 1030, 1031, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{115713, 122882, 48516, 91529, 80906, 91658, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>{70946, 2851, 5764, 4518, 26409, 7991, 1275, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>{1025, 3079, 3083, 21, 1046, 2583, 4121, 538, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>{1, 515, 261, 265, 527, 531, 21, 150, 534, 153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>606</td>\n",
       "      <td>{1, 8195, 6148, 7, 11, 69644, 4109, 15, 17, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>607</td>\n",
       "      <td>{1, 517, 2053, 2054, 1544, 3081, 11, 1036, 257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>608</td>\n",
       "      <td>{1, 4105, 10, 6157, 16, 21, 31, 32, 2080, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>609</td>\n",
       "      <td>{1, 137, 10, 650, 1161, 786, 150, 288, 161, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>610</td>\n",
       "      <td>{1, 122882, 122884, 6, 122886, 81932, 73741, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId                               movies_rated_above_2\n",
       "0         1  {1024, 1, 1025, 3, 2048, 1029, 6, 1030, 1031, ...\n",
       "1         2  {115713, 122882, 48516, 91529, 80906, 91658, 1...\n",
       "2         3  {70946, 2851, 5764, 4518, 26409, 7991, 1275, 2...\n",
       "3         4  {1025, 3079, 3083, 21, 1046, 2583, 4121, 538, ...\n",
       "4         5  {1, 515, 261, 265, 527, 531, 21, 150, 534, 153...\n",
       "..      ...                                                ...\n",
       "602     606  {1, 8195, 6148, 7, 11, 69644, 4109, 15, 17, 18...\n",
       "603     607  {1, 517, 2053, 2054, 1544, 3081, 11, 1036, 257...\n",
       "604     608  {1, 4105, 10, 6157, 16, 21, 31, 32, 2080, 34, ...\n",
       "605     609  {1, 137, 10, 650, 1161, 786, 150, 288, 161, 10...\n",
       "606     610  {1, 122882, 122884, 6, 122886, 81932, 73741, 1...\n",
       "\n",
       "[607 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a new dataframe with all unique userId\n",
    "transactional_df = pd.DataFrame({'userId':more_than_10_movies_rated_above_2.userId.unique()})\n",
    "\n",
    "# And then just get the list of all unique subreddits they are active in, assigning it to a new column\n",
    "transactional_df['movies_rated_above_2'] = [set(more_than_10_movies_rated_above_2['movieId'].loc[more_than_10_movies_rated_above_2['userId'] == x['userId']]) \n",
    "    for _, x in transactional_df.iterrows()]\n",
    "\n",
    "transactional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 607\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique users:\", more_than_10_movies_rated_above_2[\"userId\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffbfe159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies: 8852\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique movies:\", more_than_10_movies_rated_above_2[\"movieId\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd71691",
   "metadata": {},
   "source": [
    "- As we observe, the number of unique users are reduced from 610 to 607 after preprocessing, and the number of unique movies have reduced from 9724 to 8852."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dc7ac",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c1ac1",
   "metadata": {},
   "source": [
    "- Divide the data set into 80% training set and 20% test set. Remove 20% of\n",
    "movies watched from each user and create a test set using the removed\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab3a8bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[100, 200, 300, 400, 500, 600, 700]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[100, 200, 300, 400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[300, 400, 500, 600, 700, 800]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[500, 600, 700, 800, 900, 1000, 1100, 1200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[700, 800, 900]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId                         movies_rated_above_2\n",
       "0       1          [100, 200, 300, 400, 500, 600, 700]\n",
       "1       2                         [100, 200, 300, 400]\n",
       "2       4               [300, 400, 500, 600, 700, 800]\n",
       "3       6  [500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
       "4       8                              [700, 800, 900]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy data\n",
    "dummy_df = pd.DataFrame({'userId':[1,2,4,6,8], 'movies_rated_above_2':[[100,200,300,400,500,600,700], [100,200,300,400], [300,400,500,600,700,800], [500,600,700,800,900,1000,1100,1200], [700,800,900]]})\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c643aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- Parse through each user\\n- Randomly shuffle the items in the list and split into 80-20\\n- extract 20 of each user and make a separate df\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dividing dummy df into 80-20 train-test, such that 20% of movies watched from each user is test set.\n",
    "'''\n",
    "- Parse through each user\n",
    "- Randomly shuffle the items in the list and split into 80-20\n",
    "- extract 20 of each user and make a separate df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5803c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400, 500, 600, 700]\n",
      "-----\n",
      "test_list [500, 400]\n",
      "-----\n",
      "train_list [100, 200, 300, 600, 700]\n",
      "******************************************************\n",
      "[100, 200, 300, 400]\n",
      "-----\n",
      "test_list [400]\n",
      "-----\n",
      "train_list [100, 200, 300]\n",
      "******************************************************\n",
      "[300, 400, 500, 600, 700, 800]\n",
      "-----\n",
      "test_list [800, 700]\n",
      "-----\n",
      "train_list [300, 400, 500, 600]\n",
      "******************************************************\n",
      "[500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
      "-----\n",
      "test_list [1000, 500]\n",
      "-----\n",
      "train_list [600, 700, 800, 900, 1100, 1200]\n",
      "******************************************************\n",
      "[700, 800, 900]\n",
      "-----\n",
      "test_list [700]\n",
      "-----\n",
      "train_list [800, 900]\n",
      "******************************************************\n"
     ]
    }
   ],
   "source": [
    "cols = ['userId', 'movies_rated_above_2']\n",
    "train_df = pd.DataFrame(columns=cols)\n",
    "test_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "# loop through the rows using iterrows()\n",
    "for index, row in dummy_df.iterrows():\n",
    "    # print(row['userId'], row['movies_rated_above_2'])\n",
    "    print(row['movies_rated_above_2'])\n",
    "    print(\"-----\")\n",
    "    n = int(np.ceil(0.2 * len(row['movies_rated_above_2'])))  # initialize a value that represents 20% of the total items in the list.\n",
    "    test_list = random.sample(row['movies_rated_above_2'], n)  # randomly choose 20% of the values (n) from list and make a sublist.\n",
    "    print(\"test_list\", test_list)\n",
    "    print(\"-----\")\n",
    "    train_list = [i for i in row['movies_rated_above_2'] if i not in test_list] # rest 80% values of list is in train/-list\n",
    "    print(\"train_list\", train_list) # randomly choose 20% of the values from list and make a sublist\n",
    "    print(\"******************************************************\")\n",
    "    \n",
    "    df_1 = pd.DataFrame({\n",
    "    'userId': [row['userId']],\n",
    "    'movies_rated_above_2': [train_list]\n",
    "    })\n",
    "\n",
    "    df_2 = pd.DataFrame({\n",
    "    'userId': [row['userId']],\n",
    "    'movies_rated_above_2': [test_list]\n",
    "    })\n",
    "\n",
    "    train_df = pd.concat([train_df, df_1])\n",
    "    test_df = pd.concat([test_df, df_2])\n",
    "    # print(\"index\", index)\n",
    "    # train_df.loc[index].userId = row['userId']\n",
    "    # train_df.loc[index].movies_rated_above_2 = train_list\n",
    "\n",
    "    # test_df.loc[index].userId = row['userId']\n",
    "    # test_df.loc[index].movies_rated_above_2 = test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2880e839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[100, 200, 300, 400, 500, 600, 700]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[100, 200, 300, 400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[300, 400, 500, 600, 700, 800]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[500, 600, 700, 800, 900, 1000, 1100, 1200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[700, 800, 900]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId                         movies_rated_above_2\n",
       "0       1          [100, 200, 300, 400, 500, 600, 700]\n",
       "1       2                         [100, 200, 300, 400]\n",
       "2       4               [300, 400, 500, 600, 700, 800]\n",
       "3       6  [500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
       "4       8                              [700, 800, 900]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "784a0f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[100, 200, 300, 600, 700]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[100, 200, 300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[300, 400, 500, 600]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[600, 700, 800, 900, 1100, 1200]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[800, 900]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userId              movies_rated_above_2\n",
       "0      1         [100, 200, 300, 600, 700]\n",
       "0      2                   [100, 200, 300]\n",
       "0      4              [300, 400, 500, 600]\n",
       "0      6  [600, 700, 800, 900, 1100, 1200]\n",
       "0      8                        [800, 900]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "45b3982a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[500, 400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[800, 700]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[1000, 500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[700]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userId movies_rated_above_2\n",
       "0      1           [500, 400]\n",
       "0      2                [400]\n",
       "0      4           [800, 700]\n",
       "0      6          [1000, 500]\n",
       "0      8                [700]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55f6a7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75c8de",
   "metadata": {},
   "source": [
    "- Divide the data set into 80% training set and 20% test set. Remove 20% of\n",
    "movies watched from each user and create a test set using the removed\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "14e1b45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{1024, 1, 1025, 3, 2048, 1029, 6, 1030, 1031, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{115713, 122882, 48516, 91529, 80906, 91658, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>{70946, 2851, 5764, 4518, 26409, 7991, 1275, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>{1025, 3079, 3083, 21, 1046, 2583, 4121, 538, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>{1, 515, 261, 265, 527, 531, 21, 150, 534, 153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>606</td>\n",
       "      <td>{1, 8195, 6148, 7, 11, 69644, 4109, 15, 17, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>607</td>\n",
       "      <td>{1, 517, 2053, 2054, 1544, 3081, 11, 1036, 257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>608</td>\n",
       "      <td>{1, 4105, 10, 6157, 16, 21, 31, 32, 2080, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>609</td>\n",
       "      <td>{1, 137, 10, 650, 1161, 786, 150, 288, 161, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>610</td>\n",
       "      <td>{1, 122882, 122884, 6, 122886, 81932, 73741, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId                               movies_rated_above_2\n",
       "0         1  {1024, 1, 1025, 3, 2048, 1029, 6, 1030, 1031, ...\n",
       "1         2  {115713, 122882, 48516, 91529, 80906, 91658, 1...\n",
       "2         3  {70946, 2851, 5764, 4518, 26409, 7991, 1275, 2...\n",
       "3         4  {1025, 3079, 3083, 21, 1046, 2583, 4121, 538, ...\n",
       "4         5  {1, 515, 261, 265, 527, 531, 21, 150, 534, 153...\n",
       "..      ...                                                ...\n",
       "602     606  {1, 8195, 6148, 7, 11, 69644, 4109, 15, 17, 18...\n",
       "603     607  {1, 517, 2053, 2054, 1544, 3081, 11, 1036, 257...\n",
       "604     608  {1, 4105, 10, 6157, 16, 21, 31, 32, 2080, 34, ...\n",
       "605     609  {1, 137, 10, 650, 1161, 786, 150, 288, 161, 10...\n",
       "606     610  {1, 122882, 122884, 6, 122886, 81932, 73741, 1...\n",
       "\n",
       "[607 rows x 2 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extending the operations of dummy data on the original data\n",
    "\n",
    "transactional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00ec9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing transactional_df df into 80-20 train-test, such that 20% of movies watched from each user is test set.\n",
    "'''\n",
    "- Parse through each user\n",
    "- Randomly shuffle the items in the list and split into 80-20\n",
    "- extract 20 of each user and make a separate df\n",
    "'''\n",
    "\n",
    "cols = ['userId', 'movies_rated_above_2']\n",
    "train_df = pd.DataFrame(columns=cols)\n",
    "test_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# loop through the rows using iterrows()\n",
    "for index, row in transactional_df.iterrows():\n",
    "    # print(row['userId'], row['movies_rated_above_2'])\n",
    "    # print(row['movies_rated_above_2'])\n",
    "    # print(\"-----\")\n",
    "    n = int(np.ceil(0.2 * len(row['movies_rated_above_2']))) # initialize a value that represents 20% of the total items in the list.\n",
    "    test_list = random.sample(row['movies_rated_above_2'], n)  # randomly choose 20% of the values (n) from list and make a sublist.\n",
    "    # print(\"test_list\", test_list)\n",
    "    # print(\"-----\")\n",
    "    train_list = [i for i in row['movies_rated_above_2'] if i not in test_list] # rest 80% values of list is in train_list\n",
    "    # print(\"train_list\", train_list)\n",
    "    # print(\"******************************************************\")\n",
    "    \n",
    "    df_1 = pd.DataFrame({\n",
    "    'userId': [row['userId']],\n",
    "    'movies_rated_above_2': [train_list]\n",
    "    })\n",
    "\n",
    "    df_2 = pd.DataFrame({\n",
    "    'userId': [row['userId']],\n",
    "    'movies_rated_above_2': [test_list]\n",
    "    })\n",
    "\n",
    "    train_df = pd.concat([train_df, df_1])\n",
    "    test_df = pd.concat([test_df, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "866479aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1024, 1, 1025, 1029, 6, 1030, 1031, 1032, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[115713, 122882, 48516, 80906, 91658, 131724, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[70946, 2851, 5764, 26409, 7991, 1275, 2288, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[1025, 3079, 3083, 21, 1046, 2583, 4121, 538, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[1, 515, 261, 265, 527, 531, 150, 534, 153, 41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>606</td>\n",
       "      <td>[1, 8195, 11, 69644, 4109, 15, 17, 2065, 2073,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>607</td>\n",
       "      <td>[1, 517, 2054, 1544, 3081, 11, 1036, 2571, 527...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>608</td>\n",
       "      <td>[1, 4105, 6157, 16, 21, 31, 2080, 34, 2083, 41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609</td>\n",
       "      <td>[1, 137, 10, 650, 1161, 786, 150, 288, 161, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610</td>\n",
       "      <td>[1, 122882, 122886, 81932, 122892, 114707, 122...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId                               movies_rated_above_2\n",
       "0       1  [1024, 1, 1025, 1029, 6, 1030, 1031, 1032, 205...\n",
       "0       2  [115713, 122882, 48516, 80906, 91658, 131724, ...\n",
       "0       3  [70946, 2851, 5764, 26409, 7991, 1275, 2288, 8...\n",
       "0       4  [1025, 3079, 3083, 21, 1046, 2583, 4121, 538, ...\n",
       "0       5  [1, 515, 261, 265, 527, 531, 150, 534, 153, 41...\n",
       "..    ...                                                ...\n",
       "0     606  [1, 8195, 11, 69644, 4109, 15, 17, 2065, 2073,...\n",
       "0     607  [1, 517, 2054, 1544, 3081, 11, 1036, 2571, 527...\n",
       "0     608  [1, 4105, 6157, 16, 21, 31, 2080, 34, 2083, 41...\n",
       "0     609  [1, 137, 10, 650, 1161, 786, 150, 288, 161, 10...\n",
       "0     610  [1, 122882, 122886, 81932, 122892, 114707, 122...\n",
       "\n",
       "[607 rows x 2 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "977e001f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movies_rated_above_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1136, 1196, 235, 1473, 1258, 1348, 423, 3703,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[79132, 8798, 106782, 91529, 3578, 46970]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[3024, 5181, 6835, 4518]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[648, 45, 3809, 912, 595, 1885, 475, 348, 2692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[364, 247, 594, 21, 344, 58, 349, 290]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>606</td>\n",
       "      <td>[2409, 8873, 4979, 4226, 27722, 1416, 1704, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>607</td>\n",
       "      <td>[1387, 724, 3109, 2028, 2053, 1918, 208, 204, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>608</td>\n",
       "      <td>[5049, 741, 2231, 6377, 837, 608, 832, 2841, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609</td>\n",
       "      <td>[185, 356, 892, 339, 454, 589, 110, 318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610</td>\n",
       "      <td>[73676, 27808, 55280, 3671, 1213, 63113, 1261,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId                               movies_rated_above_2\n",
       "0       1  [1136, 1196, 235, 1473, 1258, 1348, 423, 3703,...\n",
       "0       2          [79132, 8798, 106782, 91529, 3578, 46970]\n",
       "0       3                           [3024, 5181, 6835, 4518]\n",
       "0       4  [648, 45, 3809, 912, 595, 1885, 475, 348, 2692...\n",
       "0       5             [364, 247, 594, 21, 344, 58, 349, 290]\n",
       "..    ...                                                ...\n",
       "0     606  [2409, 8873, 4979, 4226, 27722, 1416, 1704, 37...\n",
       "0     607  [1387, 724, 3109, 2028, 2053, 1918, 208, 204, ...\n",
       "0     608  [5049, 741, 2231, 6377, 837, 608, 832, 2841, 1...\n",
       "0     609           [185, 356, 892, 339, 454, 589, 110, 318]\n",
       "0     610  [73676, 27808, 55280, 3671, 1213, 63113, 1261,...\n",
       "\n",
       "[607 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9499fa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "180\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "# let's confirm if the first row of the transactional data has been split into 80-20.\n",
    "print(len(transactional_df[\"movies_rated_above_2\"].iloc[0]))\n",
    "print(len(train_df[\"movies_rated_above_2\"].iloc[0]))\n",
    "print(len(test_df[\"movies_rated_above_2\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a6ddb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70384d33",
   "metadata": {},
   "source": [
    "- Saving the 80% of training data and 20% of test data in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f7a345a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./output/transactional_df_train.csv')\n",
    "test_df.to_csv('./output/transactional_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf465d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649571b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469cd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe16ef-a6d6-41cf-a729-5602df1717fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                 # Importing for panel data analysis\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt                                     # Importing pyplot interface of matplotlib\n",
    "import seaborn as sns                                               # Importing seaborn library for interactive visualization\n",
    "%matplotlib inline\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import warnings                                                     # Importing warning to disable runtime warnings\n",
    "warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf074d8f-2248-4d31-ba3d-8caa0154719b",
   "metadata": {},
   "source": [
    "<a name = Section1></a>\n",
    "#### **2. Data Acquisition and Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e47576-bae7-488b-b651-caad366f1678",
   "metadata": {},
   "source": [
    "Lets analyze the dataset and identify what attributes require generalization/categorization before we perform BUC on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7481b94-5be2-43e9-b78d-9cb4040cdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(f'./data/master.xlsx') # Load the Excel dataset\n",
    "print('Shape of the dataset:', data.shape)\n",
    "data.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da7e14-44fb-43c3-9d5a-f6338aefb11a",
   "metadata": {},
   "source": [
    "- We have 27820 records and 12 attributes.\n",
    "- In our records, we have variety of data including nominal data, binomial data, numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7872d5-899a-4534-937f-9ebc62bcce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5249ec2-ef64-49b6-b6fa-c61a463dddf2",
   "metadata": {},
   "source": [
    "Checking for:\n",
    "1. duplicate values in rows - delete duplicate rows\n",
    "2. missing values in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e217a-fb6b-4ae4-8260-7141ce9576e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = data[data.duplicated()] # Selecting duplicate rows except first occurrence based on all columns\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f12f07-e6fa-4a58-8547-9269bb0a9ebf",
   "metadata": {},
   "source": [
    "- It means there are no duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ba97c-9c19-4093-94f2-33d0c7ec1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())  # Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff73097-646f-429b-b324-9fd8a73da348",
   "metadata": {},
   "source": [
    "- As we can observe HDI has got 19,456 null values, out of total 27,820 entries. Given, more than half of the entries having NULL values, let's discount this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c8b09-1774-4921-a9bc-562ed8679a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['HDI for year'], axis=1, inplace=True)   # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862504e-1638-4bc1-a5d3-c21345738005",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns # remaining columns in our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d0285-d742-4751-8c75-ad5a32b6293b",
   "metadata": {},
   "source": [
    "<a name = Section2></a>\n",
    "#### **3. Data Analysis and AOI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392215e5-e5aa-40d8-8ac3-452423f66d7f",
   "metadata": {},
   "source": [
    "Now, let's one by one, analyze the 11 dimensions and determine for which dimensions, we need to perform Attribute Oriented Induction (AOI) for generalization/categorization.\n",
    "\n",
    "Data generalization summarizes data by replacing relatively low-level values with higher-level concepts, or by reducing the number of dimensions to summarize data in concept space involving fewer dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362f451-a8ac-4b77-8742-333046bfcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn country:\\n\", data[\"country\"].unique())\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"country\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a671d-1cd7-4ebc-8533-b04c088c89b3",
   "metadata": {},
   "source": [
    "- We will use the values of 'country' dimension as it is, because it is already in the highest-level of concept hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d67cc-9b24-4261-bb1c-4f15372694f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn year:\\n\", data[\"year\"].unique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"year\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48feeb-115b-4652-b64e-b5f1d26ad918",
   "metadata": {},
   "source": [
    "- We will use the values of 'year' dimension as it has 32 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a84ad-d925-4906-9099-56d6719a5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn sex:\\n\", data[\"sex\"].unique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"sex\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b556553-513a-4473-ab56-028489365cd3",
   "metadata": {},
   "source": [
    "- We will use the values of 'sex' attribute/dimension as it is because it is already generalized, having two distinct values of the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f042464-4e2d-4b3e-894d-de8f6b2eab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn age:\\n\", data[\"age\"].unique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"age\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6295aef-3873-4a33-a65c-00f83aade194",
   "metadata": {},
   "source": [
    "- We will use the values of 'age' dimension as it is because it is already characterized by six distinct values of the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b63f93-32af-48d1-b8b1-d3b55c89cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn suicide_no:\\n\", data[\"suicides_no\"].unique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"suicides_no\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430c2af-8f75-4642-8c77-18d50c9bba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"suicides_no\"].describe()    # describe the values of the suicides_no attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536191d-0c48-41ac-a0eb-affa7d7619b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"suicides_no\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a01da-da1d-49fb-b38a-206a83d4236a",
   "metadata": {},
   "source": [
    "- We can derive from the describe() that, the minimum number of suicides are 0. The maximum number of suicide value is 22338.\n",
    "At 25th percentile, the suicide value is 3. This means that 25 percent of data that lies below this 25th percentile point will have value equal to or less than 3.\n",
    "- At 50th percentile, the suicide value is 25. This means half of the data points below 50th percentile point will have value equal to or less than 25. For the high-level description purpose, we can label all those values as `low_suicide_range`.\n",
    "- At 75th percentile, the suicide value is 131. This means that 75% of the data points that lies below this 75th percentile point will have value equal to or less than 131. For the high-level description purpose, we can label all those values above `low_suicide_range` and below the value at 75th percentile as `medium_suicide_range`.\n",
    "- Similarly, the maximum suicide number reported is 22338. All values that lie between 75th percentile value to the maximum reported value can be termed as `high_suicide_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366535b-abd4-4d58-8392-b9f6cddd365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels and conditions\n",
    "conditions = [\n",
    "    (data['suicides_no'] <= data['suicides_no'].quantile(0.5)),\n",
    "    (data['suicides_no'] > data['suicides_no'].quantile(0.5)) & (data['suicides_no'] <= data['suicides_no'].quantile(0.75)),\n",
    "    (data['suicides_no'] > data['suicides_no'].quantile(0.75))\n",
    "]\n",
    "\n",
    "labels = ['low_suicides_range', 'medium_suicides_range', 'high_suicides_range']\n",
    "\n",
    "# Create a new column with the labels\n",
    "data['suicides_range'] = np.select(conditions, labels, default='unknown')\n",
    "data.drop(['suicides_no'], axis=1, inplace=True)   # Remove the column 'suicides_no' because we are using 'suicides_range' in place of that.\n",
    "# Display the first few rows of the DataFrame with the new column\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ce0c8-2a07-4974-a65d-1a91be084b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in column population:\\n\", data[\"population\"].unique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"population\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286a120-48ee-4db0-8989-ffb39344bef4",
   "metadata": {},
   "source": [
    "- \"population\" can't be used directly. Need to perform AOI to create higher-level descriptions or categories for numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae67545-4cc9-4548-83fb-1ac8f239bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"population\"].describe().apply(lambda x: format(x, 'f')) # Suppress Scientific Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a1256-e997-4081-a439-59537daf7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"population\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cddce-4080-4d74-ab6e-d6352e878b0d",
   "metadata": {},
   "source": [
    "- We can derive from the describe() that, the minimum reported population is 278. The maximum reported population is 43805214.\n",
    "- At 25th percentile, the reported population value is 97498.5. This means that 25 percent of data that lies below this 25th percentile point will have value equal to or less than 97498.\n",
    "- At 50th percentile, the population value is 430150. This means half of the data points below 50th percentile point will have value equal to or less than 430150.\n",
    "- At 75th percentile, the population value is 1486143.25. This means that 75% of the data points that lies below this 75th percentile point will have value equal to or less than 1486143.\n",
    "- For the high-level description purpose:\n",
    "    - we can label all those values that lie between 0 and 25th percentile value as `low_population_range`.\n",
    "    - all the values that lie between 25th percentile value and 75th percentile value as `medium_population_range`.\n",
    "    - all values that lie between 75th percentile value to the maximum reported value can be termed as `high_population_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb677c-18f2-4d36-9b8c-2346451f10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels and conditions\n",
    "conditions = [\n",
    "    (data['population'] <= data['population'].quantile(0.25)),\n",
    "    (data['population'] > data['population'].quantile(0.25)) & (data['population'] <= data['population'].quantile(0.75)),\n",
    "    (data['population'] > data['population'].quantile(0.75))\n",
    "]\n",
    "\n",
    "labels = ['low_population_range', 'medium_population_range', 'high_population_range']\n",
    "\n",
    "# Create a new column with the labels\n",
    "data['population_range'] = np.select(conditions, labels, default='unknown')\n",
    "data.drop(['population'], axis=1, inplace=True)   # Remove the column 'population' because we are using 'population_range' in place of that.\n",
    "# Display the first few rows of the DataFrame with the new column\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab410499-ae96-40fa-b3f5-a1dad1fe28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"suicides/100k pop\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e6827-85ed-4485-b16b-d3648f0c4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in the column \\\"suicides/100k pop\\\":\\n\", data[\"suicides/100k pop\"].unique())\n",
    "print(\"Number of unique values:\", data[\"suicides/100k pop\"].nunique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Unique values in the column \\\"country-year\\\":\\n\", data[\"country-year\"].unique())\n",
    "print(\"Number of unique values:\", data[\"country-year\"].nunique())\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3303f6-c453-4c37-82df-ad92d495c9b6",
   "metadata": {},
   "source": [
    "- As we are already using 'population' and 'suicides_no' attributes in their generalized form, we can remove the dimension \"suicides/100k pop\" from our dataset.\n",
    "- Similarly, we are using distinct values in 'country' and 'year' dimension, therefore, we will drop the dimension 'country-year'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efef144-6aef-48c2-9f31-edfccae23511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['suicides/100k pop'], axis=1, inplace=True)   # Remove the mentioned column\n",
    "data.drop(['country-year'], axis=1, inplace=True)   # Remove the mentioned column\n",
    "data.drop(['gdp_per_capita ($)'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c974b26-1b8a-4b22-9d26-a0ca1ac5aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\" gdp_for_year ($) \"].describe().apply(lambda x: format(x, 'f')) # Suppress Scientific Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d15d76-8f74-41ab-af5c-4df3a43df13d",
   "metadata": {},
   "source": [
    "- We can derive from the describe() that, the minimum reported gdp_per_year value is 46,919,625\\\\$. The maximum reported population is 18,120,714,000,000\\\\$.\n",
    "- At 25th percentile, the reported gdp_per_year value is 8,985,352,832\\\\$. This means that 25 percent of data that lies below this 25th percentile point will have value equal to or less than 8,985,352,832\\\\$.\n",
    "- At 50th percentile, the gdp_per_year value is 48,114,688,201\\\\$. This means half of the data points below 50th percentile point will have value equal to or less than 48,114,688,201\\\\$.\n",
    "- At 75th percentile, the gdp_per_year value is 260,202,429,150\\\\$. This means that 75\\% of the data points that lies below this 75th percentile point will have value equal to or less than 260,202,429,150\\\\$. \n",
    "- For the high-level description purpose:\n",
    "    - we can label all those values that lie between 0 and 25th percentile value as `low_income_range`.\n",
    "    - all the values that lie between 25th percentile value and 75th percentile value as `medium_income_range`.\n",
    "    - all values that lie between 75th percentile value to the maximum reported value can be termed as `high_income_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a244244-5b77-4695-b70d-8a061ae388d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels and conditions\n",
    "conditions = [\n",
    "    (data[' gdp_for_year ($) '] <= data[' gdp_for_year ($) '].quantile(0.25)),\n",
    "    (data[' gdp_for_year ($) '] > data[' gdp_for_year ($) '].quantile(0.25)) & (data[' gdp_for_year ($) '] <= data[' gdp_for_year ($) '].quantile(0.75)),\n",
    "    (data[' gdp_for_year ($) '] > data[' gdp_for_year ($) '].quantile(0.75))\n",
    "]\n",
    "\n",
    "labels = ['low_income_range', 'medium_income_range', 'high_income_range']\n",
    "\n",
    "# Create a new column with the labels\n",
    "data['gdp_per_year_income_range'] = np.select(conditions, labels, default='unknown')\n",
    "data.drop([' gdp_for_year ($) '], axis=1, inplace=True)   # Remove the column 'gdp+per_year ($)' because we are using 'gdp_per_year_income_range' in place of that.\n",
    "# Display the first few rows of the DataFrame with the new column\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab6f56-3ccb-41f7-9dd6-59502a654324",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"generation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654c187-4686-4b4e-9dbf-69fce1de5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values:\\n\", data[\"generation\"].unique())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Number of unique values:\", data[\"generation\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2f327-ec22-430d-be92-6fa83bb91f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92266f-5fd8-441d-9eb4-7e8d07c8b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete unnecessary variabels after Preprocessing stesps\n",
    "del conditions, duplicate, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb06d96-89e2-4938-9a31-1ab1721eae0f",
   "metadata": {},
   "source": [
    "<a name = Section4></a>\n",
    "#### **4. BUC Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04067b10-d9cb-48ef-b692-754ebbe0d99d",
   "metadata": {},
   "source": [
    "- The cell below contains a function to encode categorical attributes to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5188bf1-2560-40d5-9e9d-30fa6c3a78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_df:\n",
    "  '''\n",
    "  Class to preprocess DataFrame\n",
    "  '''\n",
    "  def encode_attributes(self, input_df, column_indices):\n",
    "    transformed_dicts_ls = []\n",
    "    transformed_df = input_df.copy(deep = True)\n",
    "    column_names = transformed_df.columns.tolist()\n",
    "    for col_iter in column_indices:\n",
    "      temp_dict = {}\n",
    "      temp_key = 0\n",
    "      temp_ls = []\n",
    "      column_name = column_names[col_iter]\n",
    "      for col in transformed_df.iloc[:,col_iter].tolist():\n",
    "        if col not in [*temp_dict.keys()]:\n",
    "          temp_dict[col] = temp_key\n",
    "          temp_key += 1\n",
    "        temp_ls.append(temp_dict[col])\n",
    "      dict_inv = {v:k for k,v in temp_dict.items()}\n",
    "      transformed_dicts_ls.append(dict_inv)\n",
    "      transformed_df[column_name] = temp_ls\n",
    "    return transformed_df, transformed_dicts_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e2177-2817-428d-a77f-b2a2a02a9e68",
   "metadata": {},
   "source": [
    "#### When the entire dataframe fits into the main memory\n",
    "- Below cell contains the class for BUC implementation when the dataframe fits into the main memory.\n",
    "- Most of the variable names are exactly as indicated in the paper.\n",
    "- Counting sort is used as prescribed in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733c2da-62a2-44a5-9dc1-9089e8b43430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class buc:\n",
    "    '''\n",
    "    Class for implementing BUC\n",
    "    '''\n",
    "    def __init__(self, df, column_enc_dicts_ls, minsup):\n",
    "        self.numDims = df.shape[1]\n",
    "        self.cardinality = []\n",
    "        self.minsup = minsup\n",
    "        self.output_df = None\n",
    "        self.datacounts = [[]] * df.shape[1]\n",
    "        self.attribute_ls = [\"*\"] * df.shape[1]\n",
    "        self.debug_counter = 0\n",
    "        self.output_dict = {}\n",
    "        self.column_enc_dicts_ls = column_enc_dicts_ls\n",
    "\n",
    "    def counting_sort(self, array_a, df_idx_ls):\n",
    "      '''\n",
    "      Inputs \n",
    "      array_a: List to be sorted\n",
    "      df_idx_ls: Index list corresponding to the array_a. For example: DataFrame indices corresponding to array_a.\n",
    "      Output\n",
    "      idx_ls: Order in which df_idx_ls should be arranged so that array_a is in the sorted order i.e argsorting array_a\n",
    "      '''\n",
    "      array_c = [0]*(max(array_a) + 1)\n",
    "      idx_ls = [-1] * (len(array_a))\n",
    "\n",
    "      for i in range(0, len(array_a)):\n",
    "        array_c[array_a[i]] += 1\n",
    "\n",
    "      for i in range(0, len(array_c) - 1):\n",
    "        array_c[i+1] = array_c[i] + array_c[i+1]\n",
    "\n",
    "      for i in range(len(array_a) - 1, -1, -1):\n",
    "        array_c[array_a[i]] = array_c[array_a[i]] - 1\n",
    "        idx = array_c[array_a[i]]\n",
    "        idx_ls[idx] = df_idx_ls[i]\n",
    "\n",
    "      return idx_ls\n",
    "\n",
    "\n",
    "    def partition(self, input_df, d):\n",
    "        '''\n",
    "        Implements partitioning logic i.e sorts the input dataframe and populates self.datacounts\n",
    "        Inputs:\n",
    "        input_df: Input DataFrame\n",
    "        d: column number based on which sorting is performed\n",
    "        Output:\n",
    "        input_df: DataFrame which is sorted according to the specified column\n",
    "        '''\n",
    "        #Sorting the dataframe\n",
    "        temp_counter_dict = {}\n",
    "        sorted_idx = self.counting_sort(input_df.iloc[:,d].tolist(), input_df.index.tolist())\n",
    "        input_df = input_df.reindex(sorted_idx)\n",
    "        #Populating self.datacounts\n",
    "        for attribute in input_df.iloc[:,d].tolist():\n",
    "            temp_counter_dict[attribute] = temp_counter_dict.get(attribute, 0) + 1\n",
    "        self.datacounts[d] = [*temp_counter_dict.values()]\n",
    "        return input_df\n",
    "\n",
    "    def buc_implementation(self, input, dim):\n",
    "        '''\n",
    "        Function to implement BUC as indicated in the original paper. \n",
    "        Populates self.output_dict which is the output dictionary.\n",
    "        NOTE:Most of the variable names are exactly as indicated in the original paper.\n",
    "        Input\n",
    "        input: Input DataFrame\n",
    "        dim: Starting column for performing aggregation\n",
    "        '''\n",
    "        self.debug_counter += 1\n",
    "        if tuple(self.attribute_ls) in [*self.output_dict.keys()]:\n",
    "          print(f\"Error!!\")\n",
    "        self.output_dict[tuple(self.attribute_ls)] = input.shape[0]\n",
    "       \n",
    "        for d in range(dim, self.numDims,1):\n",
    "            bigc = input.iloc[:,d].nunique()\n",
    "            input = self.partition(input, d)\n",
    "            k = 0\n",
    "            for i in range(0, bigc, 1):\n",
    "                smallc = self.datacounts[d][i]\n",
    "                if smallc >= self.minsup:\n",
    "                    self.attribute_ls[d] = self.column_enc_dicts_ls[d][input.iloc[k,d]]\n",
    "                    self.buc_implementation(input.iloc[k:k+smallc,:], dim=d+1)\n",
    "                k += smallc\n",
    "            self.attribute_ls[d] = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52681a-75dc-4d6e-bfee-3279f4a78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(output_dict):\n",
    "    '''\n",
    "    Function that takes the output dictionary and converts it into a dataframe for readability\n",
    "    '''\n",
    "    output_dict_transformed = {}\n",
    "    columns_ls = input_df.columns.tolist()\n",
    "    for column in columns_ls:\n",
    "        output_dict_transformed[column] = []\n",
    "        output_dict_transformed['count'] = []\n",
    "    for tuple_key, value in output_dict.items():\n",
    "        output_dict_transformed['count'].append(value)\n",
    "        for tuple_key_iter in range(0,len(tuple_key)):\n",
    "            output_dict_transformed[columns_ls[tuple_key_iter]].append(tuple_key[tuple_key_iter])\n",
    "    output_df = pd.DataFrame.from_dict(output_dict_transformed)\n",
    "    columns_order = ['country', 'year', 'sex', 'age', 'generation', 'suicides_range', 'population_range', 'gdp_per_year_income_range', 'count']\n",
    "    output_df = output_df.reindex(columns = columns_order)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c9d2f-dede-4bc1-b214-1ae3f7505b87",
   "metadata": {},
   "source": [
    "- BUC Output when the entire dataframe fits into the main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244659d-62cc-42c7-83be-5f46d617c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUC Implementation: When the entire dataframe fits into the main memory.\n",
    "minsup = 100\n",
    "num_splits = 10\n",
    "input_df = data\n",
    "#Encode all of the categorical attributes into numerical form.\n",
    "preprocess_obj = preprocess_df()\n",
    "transformed_df, column_enc_dicts_ls = preprocess_obj.encode_attributes(input_df, [*range(0,input_df.shape[1])]) #NOTE: This should be modified as required\n",
    "buc_obj = buc(transformed_df, column_enc_dicts_ls, minsup)\n",
    "buc_obj.buc_implementation(transformed_df, 0)\n",
    "output_dict = buc_obj.output_dict\n",
    "format_output(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30811611-18d8-4906-ad46-dd8ef83c538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del minsup, num_splits, input_df, preprocess_obj, transformed_df, column_enc_dicts_ls, buc_obj, output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2626f0-c134-4cfa-8bc6-38ec7cb8e318",
   "metadata": {},
   "source": [
    "#### When the entire dataframe doesn't fit into the main memory\n",
    "- The dataframe is partitioned into a certain number of chunks which are saved on the disk.\n",
    "- External merge sort is implemented for partitioning.\n",
    "- For external merge sort, we perform a k-way merge after sorting each individual list (assuming that we are given k lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582f75f-c4f4-441f-a966-f02b4fffd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_merge_sort_implementation(chunk_paths, d, iteration_num, index_dict, file_path):\n",
    "    '''\n",
    "    Implements external merge sort i.e to sort k dataframes when all of them don't fit in the main memory.\n",
    "    Given K chunks, the function replaces the unsorted chunks with their globally sorted versions.\n",
    "    Inputs:\n",
    "    chunk_paths: Locations of each chunk\n",
    "    d: Column number on which sorting is to be performed\n",
    "    iteration_num: Iteration number\n",
    "    index_dict: Dictionary containing the indices in each chunked dataframe.\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    sorted_chunk_paths = []\n",
    "    index_ls = copy.deepcopy([*index_dict.values()])\n",
    "    output_len_ls = copy.deepcopy([*index_dict.values()])\n",
    "    index_ls = [len(i) for i in index_ls]\n",
    "    output_len_ls = [len(i) for i in output_len_ls]\n",
    "    output_len_idx = 0\n",
    "    #Sorting each individual dataframe\n",
    "    for i_iter, chunk_path in enumerate(chunk_paths):\n",
    "        chunk_df = pd.read_pickle(chunk_path)\n",
    "        if i_iter == 0:\n",
    "            df_columns = chunk_df.columns\n",
    "        chunk_df_ls = chunk_df.values.tolist()\n",
    "        chunk_df_ls.sort(key = lambda x: (x[d]))\n",
    "        sorted_chunk_path = f\"./sorted_{chunk_path.split('/')[-1].split('.pkl')[0]}\"\n",
    "        sorted_chunk_df = pd.DataFrame(chunk_df_ls, columns=df_columns)\n",
    "        sorted_chunk_df.to_pickle(sorted_chunk_path)\n",
    "        sorted_chunk_paths.append(sorted_chunk_path)\n",
    "\n",
    "    #Merging the sorted chunks by performing a K-way merge operation\n",
    "    output_ls = []\n",
    "    idxs_ls = [0] * len(chunk_paths)\n",
    "    input_ls = []\n",
    "    min_idx = -1\n",
    "    debug_counter = 0\n",
    "    sorted_chunks_path_cpy = copy.deepcopy(sorted_chunk_paths)\n",
    "    while True:\n",
    "        #First iteration\n",
    "        if min_idx == -1:\n",
    "            for sorted_chunk_path in sorted_chunks_path_cpy:\n",
    "                df = pd.read_pickle(sorted_chunk_path)\n",
    "                input_ls.append(df.values.tolist()[idxs_ls[i_iter]])\n",
    "        else:\n",
    "            try:\n",
    "                df = pd.read_pickle(sorted_chunks_path_cpy[min_idx])\n",
    "                input_ls.insert(min_idx, df.values.tolist()[idxs_ls[min_idx]])\n",
    "            except Exception as err:\n",
    "                print(f\"Inside except block... {err}\")\n",
    "  \n",
    "        input_arr = np.array(input_ls)\n",
    "        min_idx = np.argmin(input_arr, axis=0)[d]\n",
    "        output_ls.append(input_ls[min_idx])\n",
    "        input_ls.pop(min_idx)\n",
    "        idxs_ls[min_idx] += 1\n",
    "\n",
    "        #Writing output\n",
    "        if len(output_ls) >= output_len_ls[output_len_idx]:\n",
    "            sorted_chunk_df = pd.DataFrame(output_ls, columns=df_columns)\n",
    "            sorted_chunk_df.to_pickle(f\"{file_path}{iteration_num}{output_len_idx}.pkl\")\n",
    "            output_ls = []\n",
    "            output_len_idx += 1\n",
    "        \n",
    "        if idxs_ls[min_idx] > (index_ls[min_idx] -1):\n",
    "            idxs_ls.pop(min_idx)\n",
    "            sorted_chunks_path_cpy.pop(min_idx)\n",
    "            index_ls.pop(min_idx)\n",
    "            if len(input_ls) == 0:\n",
    "                break \n",
    "            try:\n",
    "                if min_idx > len(input_ls) - 1:\n",
    "                    min_idx -= 1\n",
    "                    input_ls.pop(min_idx)\n",
    "                else:\n",
    "                    input_ls.pop(min_idx)\n",
    "            except Exception as err:\n",
    "                print(f\"{err}\")\n",
    "                \n",
    "    #Remove sorted_chunk_paths\n",
    "    for sorted_chunk in sorted_chunk_paths:\n",
    "        os.remove(sorted_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c5638-e107-451f-b50e-8da617d8f126",
   "metadata": {},
   "source": [
    "- Below cell contains the code for implementing the BUC class when the entire dataframe doesn't fit into the main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c989479-8cef-4ced-9c96-b0f3b7d514ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUC implementation\n",
    "class buc_external:\n",
    "    '''\n",
    "    Class for implementing BUC\n",
    "    '''\n",
    "    def __init__(self, df, column_enc_dicts_ls, minsup):\n",
    "        self.numDims = df.shape[1]\n",
    "        self.cardinality = []\n",
    "        self.minsup = minsup\n",
    "        self.output_df = None\n",
    "        self.datacounts = [[]] * df.shape[1]\n",
    "        self.attribute_ls = [\"*\"] * df.shape[1]\n",
    "        self.debug_counter = 0\n",
    "        self.output_dict = {}\n",
    "        self.column_enc_dicts_ls = column_enc_dicts_ls\n",
    "        self.file_path = './dfs/iter_run_'\n",
    "        os.makedirs(f\"./dfs\", exist_ok=True)\n",
    "\n",
    "    def counting_sort(self, array_a, df_idx_ls):\n",
    "      '''\n",
    "      Inputs \n",
    "      array_a: List to be sorted\n",
    "      df_idx_ls: Index list corresponding to the array_a. For example: DataFrame indices corresponding to array_a.\n",
    "      Output\n",
    "      idx_ls: Order in which df_idx_ls should be arranged so that array_a is in the sorted order.\n",
    "      '''\n",
    "      array_c = [0]*(max(array_a) + 1)\n",
    "      idx_ls = [-1] * (len(array_a))\n",
    "\n",
    "      for i in range(0, len(array_a)):\n",
    "        array_c[array_a[i]] += 1\n",
    "\n",
    "      for i in range(0, len(array_c) - 1):\n",
    "        array_c[i+1] = array_c[i] + array_c[i+1]\n",
    "\n",
    "      for i in range(len(array_a) - 1, -1, -1):\n",
    "        array_c[array_a[i]] = array_c[array_a[i]] - 1\n",
    "        idx = array_c[array_a[i]]\n",
    "        idx_ls[idx] = df_idx_ls[i]\n",
    "\n",
    "      return idx_ls\n",
    "\n",
    "    def partition(self, iteration_num, num_splits, index_dict, d):\n",
    "        '''\n",
    "        Function to perform partitioning, external merge sort is used here.\n",
    "        Inputs:\n",
    "        iteration_num: Iteration number.\n",
    "        num_splits: No. of chunks into which the input file is divided.\n",
    "        index_dict: Dictionary containing the indices in each chunked dataframe.\n",
    "        d: Attribute on which sorting is to be performed.\n",
    "        '''\n",
    "        chunk_paths_ls = []\n",
    "        for i_iter in range(0, num_splits):\n",
    "            if os.path.exists(f\"{self.file_path}{iteration_num}{i_iter}.pkl\"):\n",
    "                chunk_paths_ls.append(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                \n",
    "        external_merge_sort_implementation(chunk_paths_ls, d, iteration_num, index_dict, self.file_path)\n",
    "        \n",
    "        input_df = pd.DataFrame()\n",
    "        for i_iter in range(0, num_splits):\n",
    "            if os.path.exists(f\"{self.file_path}{iteration_num}{i_iter}.pkl\"):\n",
    "                df = pd.read_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                input_df = pd.concat([input_df, df])\n",
    "        \n",
    "        #Populating self.datacounts\n",
    "        temp_counter_dict = {}\n",
    "        for attribute in input_df.iloc[:,d].tolist():\n",
    "            temp_counter_dict[attribute] = temp_counter_dict.get(attribute, 0) + 1\n",
    "        self.datacounts[d] = [*temp_counter_dict.values()]\n",
    "        return None\n",
    "                    \n",
    "        \n",
    "    def compute_aggregate(self, iteration_num, num_splits):\n",
    "        '''\n",
    "        Function to find the count i.e the total number of rows in all the chunks combined.\n",
    "        '''\n",
    "        count = 0\n",
    "        for i_iter in range(0,num_splits):\n",
    "            if os.path.exists(f\"{self.file_path}{iteration_num}{i_iter}.pkl\"):\n",
    "                df = pd.read_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                count += df.shape[0]\n",
    "        return count\n",
    "\n",
    "    \n",
    "    def find_bigc(self, iteration_num, num_splits, d): \n",
    "        '''\n",
    "        Bigc refers to the cardinality of the dth attribute in the dataframe\n",
    "        '''\n",
    "        computed_values = []\n",
    "        bigc = 0\n",
    "        for i_iter in range(0, num_splits):\n",
    "            if os.path.exists(f\"{self.file_path}{iteration_num}{i_iter}.pkl\"):\n",
    "                df = pd.read_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                for attribute_name in df.iloc[:,d].unique().tolist():\n",
    "                    if attribute_name not in computed_values:\n",
    "                        computed_values.append(attribute_name)\n",
    "                        bigc += 1\n",
    "        return bigc\n",
    "\n",
    "    def split_input(self, slice_range, iteration_num, num_splits):\n",
    "        '''\n",
    "        Function to split the input dataframe into a specified number of chunks. \n",
    "        Chunks in iteration 't' are obtained by taking the correct indices from iteration 't-1'\n",
    "        '''\n",
    "        #Delete previous files of current iteration\n",
    "        for i_iter in range(0, num_splits):\n",
    "            if os.path.exists(f\"{self.file_path}{iteration_num}{i_iter}.pkl\"):\n",
    "                os.remove(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                            \n",
    "        index_dict_t = {} #Stores the indices that are present in each dictionary\n",
    "        #Indicates the very first iteration\n",
    "        if not os.path.exists(f\"{self.file_path}{iteration_num-1}0.pkl\"):\n",
    "            input_df = transformed_df\n",
    "            split_df = np.array_split(input_df, num_splits)\n",
    "            start_idx = 0\n",
    "            for i_iter, df in enumerate(split_df):\n",
    "                # print(f\"DF after splitting: {df}\")\n",
    "                df.to_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                index_dict_t[i_iter] = [*range(start_idx, start_idx + df.shape[0])]\n",
    "                start_idx += df.shape[0]   \n",
    "            with open(f\"{self.file_path}_dict_{iteration_num}.pkl\", \"wb\") as fp:\n",
    "                pickle.dump(index_dict_t, fp)\n",
    "        else:  \n",
    "            if (slice_range[1] - slice_range[0]) < num_splits:\n",
    "                df_lengths_t = [slice_range[1] - slice_range[0]]\n",
    "                df_lengths_t.extend([0]*(num_splits-1))\n",
    "            else:\n",
    "                df_lengths_t = [math.floor((slice_range[1] - slice_range[0])/num_splits)]*num_splits\n",
    "                df_lengths_t[-1] += (slice_range[1] - slice_range[0])%num_splits\n",
    "            df_ranges_t_ls = []\n",
    "            start_idx = slice_range[0]\n",
    "            for df_length in df_lengths_t:\n",
    "                if df_length != 0:\n",
    "                    df_ranges_t_ls.append([*range(start_idx,start_idx + df_length)])\n",
    "                    start_idx += df_length\n",
    "            del start_idx \n",
    "            \n",
    "            with open(f\"{self.file_path}_dict_{iteration_num-1}.pkl\", \"rb\") as fp:\n",
    "                index_dict_tminus = pickle.load(fp)\n",
    "\n",
    "            start_dict_idx = 0\n",
    "            index_dict_t = {}\n",
    "            for i_iter, df_range_t in enumerate(df_ranges_t_ls):\n",
    "                start_df_idx = 0\n",
    "                df = pd.DataFrame()\n",
    "                for df_tminus_name, df_tminus_range in index_dict_tminus.items():\n",
    "                    common_elements = [i for i in df_range_t for j in df_tminus_range if i == j]\n",
    "                    common_elements = [i-start_df_idx for i in common_elements]\n",
    "                    if len(common_elements) > 0:\n",
    "                        temp_df = pd.read_pickle(f\"{self.file_path}{iteration_num-1}{df_tminus_name}.pkl\")\n",
    "                        df = pd.concat([df, temp_df.iloc[common_elements,:]])\n",
    "                    start_df_idx += len(df_tminus_range)\n",
    "                #Writing output pickle files\n",
    "                df.to_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "                index_dict_t[i_iter] = [*range(start_dict_idx, start_dict_idx + df.shape[0])]\n",
    "                start_dict_idx += df.shape[0]\n",
    "                #Writing the output dictionary\n",
    "                with open(f\"{self.file_path}_dict_{iteration_num}.pkl\", \"wb\") as fp:\n",
    "                    pickle.dump(index_dict_t, fp)\n",
    "        return index_dict_t\n",
    "\n",
    "    def populate_attribute_ls(self, k, d, iteration_num):\n",
    "        '''\n",
    "        Function to populate items in the output dictionary\n",
    "        '''\n",
    "        with open(f\"{self.file_path}_dict_{iteration_num}.pkl\", \"rb\") as fp:\n",
    "            index_dict_t = pickle.load(fp)\n",
    "\n",
    "        k_idx = k\n",
    "        for df_name, df_idxs in index_dict_t.items():\n",
    "            if k in df_idxs:\n",
    "                df = pd.read_pickle(f\"{self.file_path}{iteration_num}{df_name}.pkl\")\n",
    "                self.attribute_ls[d] = self.column_enc_dicts_ls[d][df.iloc[k_idx,d]]\n",
    "                return None\n",
    "            k_idx -= len(df_idxs)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def buc_implementation(self, slice_range, dim, iteration_num, num_splits):\n",
    "        '''\n",
    "        Function to implement BUC when it is assumed that the dataframe doesn't fit in the main memory. \n",
    "        Populates self.output_dict which is the output dictionary.\n",
    "        Inputs\n",
    "        slice_range: Rows of the dataframe which have to sliced at iteration 't'.\n",
    "        dim: Starting column for performing aggregation.\n",
    "        iteration_num: Iteration number.\n",
    "        num_splits: No.of chunks into which the input dataframe is to be divided.\n",
    "        '''\n",
    "        self.debug_counter += 1\n",
    "        #Split the input into chunks, chunks at iteration 't' are computed from chunks at iteration 't-1'\n",
    "        index_dict = self.split_input(slice_range, iteration_num, num_splits)\n",
    "        #Compute count\n",
    "        aggregate = self.compute_aggregate(iteration_num, num_splits)\n",
    "        self.output_dict[tuple(self.attribute_ls)] = aggregate\n",
    "        \n",
    "        for d in range(dim, self.numDims,1):\n",
    "            bigc = self.find_bigc(iteration_num, num_splits, d)\n",
    "            #External partitioning: Calls merge sort\n",
    "            self.partition(iteration_num, num_splits, index_dict, d)\n",
    "            k = 0\n",
    "            for i in range(0, bigc, 1):\n",
    "                smallc = self.datacounts[d][i]\n",
    "                if smallc >= self.minsup:\n",
    "                    self.populate_attribute_ls(k, d, iteration_num)\n",
    "                    self.buc_implementation(slice_range = [k,k+smallc], dim=d+1, iteration_num=iteration_num+1, num_splits=num_splits)\n",
    "                k += smallc\n",
    "            self.attribute_ls[d] = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c752e-5fdc-467b-9ce5-85fc6976669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup = 100\n",
    "input_df = data\n",
    "num_splits = 20\n",
    "preprocess_obj = preprocess_df()\n",
    "transformed_df, column_enc_dicts_ls = preprocess_obj.encode_attributes(input_df, [*range(0,input_df.shape[1])]) #NOTE: This should be modified as required\n",
    "buc_obj = buc_external(transformed_df, column_enc_dicts_ls, minsup)\n",
    "buc_obj.buc_implementation([0,transformed_df.shape[0]], 0, 0, num_splits)\n",
    "output_dict = buc_obj.output_dict\n",
    "format_output(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b35ba6-988b-4de6-aade-a2f0e70d32c2",
   "metadata": {},
   "source": [
    "#### a) A plot of minsup vs. runtime, keeping allotted memory fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f2047-ed57-4e67-86bf-a3c4de868fe6",
   "metadata": {},
   "source": [
    "- Here it is assumed that the dataframe fits into the main memory.\n",
    "- The value of minsup is varied between 90 and 200 in steps of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243bbe2-647f-40c5-b84f-d4ee1a55c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "input_df = data\n",
    "#Encode all of the categorical attributes into numerical form.\n",
    "preprocess_obj = preprocess_df()\n",
    "transformed_df, column_enc_dicts_ls = preprocess_obj.encode_attributes(input_df, [*range(0,input_df.shape[1])]) #NOTE: This should be modified as required\n",
    "minsup_ls = []\n",
    "time_taken_ls = []\n",
    "for minsup in range(90, 200, 10):\n",
    "    minsup_ls.append(minsup)\n",
    "    start_time = time.time()\n",
    "    buc_obj = buc(transformed_df, column_enc_dicts_ls, minsup)\n",
    "    buc_obj.buc_implementation(transformed_df, 0)\n",
    "    time_taken_ls.append(time.time() - start_time)\n",
    "del minsup, num_splits, input_df, preprocess_obj, transformed_df, column_enc_dicts_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583575da-9d06-4a64-8bc2-70ff7ba4fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(minsup_ls, time_taken_ls)\n",
    "plt.xlabel(f\"Minimum Support\")\n",
    "plt.ylabel(f\"Runtime (seconds)\")\n",
    "plt.title(f\"Plot of Minimum support vs Runtime(seconds)\")\n",
    "del minsup_ls, time_taken_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4870e-7458-4389-8557-c573004c1f59",
   "metadata": {},
   "source": [
    "#### b) A plot of allotted memory vs. runtime, keeping minsup fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd412209-3a90-4359-a380-0423803054bb",
   "metadata": {},
   "source": [
    "- The variation in the allocated main memory is indicated by varying the number of chunks into which the original dataframe is divided i.e greater the number of chunks lesser is the main memory that is allocated.\n",
    "-  NOTE: The dataframe is restricted to 10K rows to speed up the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec3252-608a-4886-b0b7-73b4fc9b4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup = 200\n",
    "input_df = data.loc[0:10000,:]\n",
    "preprocess_obj = preprocess_df()\n",
    "transformed_df, column_enc_dicts_ls = preprocess_obj.encode_attributes(input_df, [*range(0,input_df.shape[1])]) #NOTE: This should be modified as required\n",
    "chunks_ls = []\n",
    "time_taken_ls = []\n",
    "for num_splits in range(30,50,5):\n",
    "    chunks_ls.append(num_splits)\n",
    "    start_time = time.time()\n",
    "    buc_obj = buc_external(transformed_df, column_enc_dicts_ls, minsup)\n",
    "    buc_obj.buc_implementation([0,transformed_df.shape[0]], 0, 0, num_splits)\n",
    "    time_taken_ls.append(time.time() - start_time)\n",
    "    # print(f\"{time_taken_ls = }\")\n",
    "del minsup, num_splits, input_df, preprocess_obj, transformed_df, column_enc_dicts_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb59bcc-caa5-47aa-ad55-fee1106fe3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chunks_ls, time_taken_ls)\n",
    "plt.xlabel(f\"No.of chunks into which main memory is divided\")\n",
    "plt.ylabel(f\"Time taken in seconds\")\n",
    "plt.title(f\"Plot of Number of chunks(indication of main memory) vs Runtime (seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144153b8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebb197",
   "metadata": {},
   "source": [
    "## PART B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc82cc",
   "metadata": {},
   "source": [
    "<a name = Section1></a>\n",
    "#### **1. Data Acquisition and Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cce39d",
   "metadata": {},
   "source": [
    "Lets analyze the dataset and identify what attributes require generalization/categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data = pd.read_excel('./data/cleaned_data.xlsx')      # Load the Excel dataset\n",
    "print('Shape of the dataset:', property_data.shape)\n",
    "property_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e659ee",
   "metadata": {},
   "source": [
    "- We have 143,708 records and 32 attributes.\n",
    "- In our records, we have variety of data including nominal data, binomial data, numerical data and textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.info() # Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ed886",
   "metadata": {},
   "source": [
    "- Here we show the data type of our various attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad29cb",
   "metadata": {},
   "source": [
    "Always check for:\n",
    "1. duplicate values in rows - delete duplicate rows\n",
    "2. missing values in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59faf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = property_data[property_data.duplicated()]     # Selecting duplicate rows except first occurrence based on all columns\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bf1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(property_data.isnull().sum())      # Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07cb91",
   "metadata": {},
   "source": [
    "- We observe that there are no missing and duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54835130",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.columns # columns in our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba037c",
   "metadata": {},
   "source": [
    "<a name = Section2></a>\n",
    "#### **2. Data Analysis and AOI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c0d97",
   "metadata": {},
   "source": [
    "Now, let's one by one, analyze the 32 dimensions and determine for which dimension, we need to perform Attribute Oriented Induction (AOI) for generalization/categorization.\n",
    "\n",
    "Data generalization summarizes data by replacing relatively low-level values with higher-level concepts, or by reducing the number of dimensions to summarize data in concept space involving fewer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cee8e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Property_Name':\\n\", property_data[\"Property_Name\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Property_Name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Property_id':\\n\", property_data[\"Property_id\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Property_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'builder_id':\\n\", property_data[\"builder_id\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"builder_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Builder_name':\\n\", property_data[\"Builder_name\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Builder_name\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b7e74",
   "metadata": {},
   "source": [
    "Let's drop all these columns as they don't seem to provide any significant information, also they have a lot of distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9adfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Property_Name'], axis=1, inplace=True)     # Remove the mentioned column\n",
    "property_data.drop(['Property_id'], axis=1, inplace=True)     # Remove the mentioned column\n",
    "property_data.drop(['builder_id'], axis=1, inplace=True)     # Remove the mentioned column\n",
    "property_data.drop(['Builder_name'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d54329",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b032ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Property_type':\\n\", property_data[\"Property_type\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Property_type\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88db4f",
   "metadata": {},
   "source": [
    "- We will use the values of 'Property_type' dimension as it is because it is already characterized by five distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Property_status':\\n\", property_data[\"Property_status\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Property_status\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabe3f5",
   "metadata": {},
   "source": [
    "- We will use the values of 'Property_status' dimension as it is because it is characterized by 2 distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f519f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Price_per_unit_area':\\n\", property_data[\"Price_per_unit_area\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Price_per_unit_area\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c274b40",
   "metadata": {},
   "source": [
    "- Let's remove the dimension 'Price_per_unit_area' as we already have the dimensions 'Price' and 'Size' that we will generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Price_per_unit_area'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fdb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Price':\\n\", property_data[\"Price\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Price\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83213068",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data['Price'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4674c",
   "metadata": {},
   "source": [
    "- However, it seems that the data type of **Price** is object. Let's change that to float, so that we can describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert object type to str, removing comma and then convert str dtype to float\n",
    "property_data['Price'] = property_data['Price'].astype('str') \n",
    "property_data['Price'] = property_data['Price'].str.replace(',','')\n",
    "property_data['Price'] = property_data['Price'].astype(float)\n",
    "\n",
    "property_data['Price'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data['Price'].describe().apply(lambda x: format(x, 'f')) # suppress scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfa1ee",
   "metadata": {},
   "source": [
    "- We can derive from the describe() that, the minimum value of Price is 100000. The maximum value of Price is 800000000. \n",
    "At 25th percentile, the value of Price is value is 5259437. This means that 25 percent of data that lies below this 25th percentile point will have value equal to or less than 5259437. \n",
    "- At 50th percentile, the value of Price is 8500000. This means half of the data points below 50th percentile point will have value equal to or less than 8500000.\n",
    "- At 75th percentile, the price value is 15498000. This means that 75% of the data points that lies below this 75th percentile point will have value equal to or less than 15498000. \n",
    "- For the high-level description purpose, we label all those values below 25th percentile point as `low_price_range`.\n",
    "- We label all those values above low_price_range and below the value at 75th percentile as `medium_price_range`.\n",
    "- Similarly, all values that lie between 75th percentile value to the maximum reported value can be termed as `high_price_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels and conditions\n",
    "conditions = [\n",
    "    (property_data[\"Price\"] <= property_data[\"Price\"].quantile(0.25)),\n",
    "    (property_data[\"Price\"] > property_data[\"Price\"].quantile(0.25)) & (property_data[\"Price\"] <= property_data[\"Price\"].quantile(0.75)),\n",
    "    (property_data[\"Price\"] > property_data[\"Price\"].quantile(0.75))\n",
    "]\n",
    "\n",
    "labels = ['low_price_range', 'mid_price_range', 'high_price_range']\n",
    "\n",
    "# Create a new column with the labels\n",
    "property_data[\"Price_range\"] = np.select(conditions, labels, default='unknown')\n",
    "property_data.drop(['Price'], axis=1, inplace=True)   # Remove the column 'Price' because we are using 'Price_range' in place of that.\n",
    "# Display the first few rows of the DataFrame with the new column\n",
    "property_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Size':\\n\", property_data[\"Size\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Size\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[\"Size\"].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e35b61",
   "metadata": {},
   "source": [
    "- However, it seems that the data type of **Size** is object. Let's change that to float, so that we can describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert object type to str, removing comma and letters and then convert str dtype to int\n",
    "property_data['Size'] = property_data['Size'].astype('str') \n",
    "property_data['Size'] = property_data['Size'].str.replace(',','') \n",
    "property_data['Size'] = property_data['Size'].str.extract('(^[^\\s]+)')\n",
    "property_data['Size'] = property_data['Size'].astype(int)\n",
    "property_data['Size'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[\"Size\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdff03",
   "metadata": {},
   "source": [
    "- We can derive from the describe() that, the minimum value of Size is 100 sq. ft.. The maximum value of Size is 90000 sq. ft.. \n",
    "At 25th percentile, the value of Size is value is 720 sq. ft. This means that 25 percent of the entire data that lies below this 25th percentile point will have value equal to or less than 720 sq. ft. \n",
    "- At 50th percentile, the value of Size is 1076 sq ft. This means half of the data points below 50th percentile point will have value equal to or less than 1076.\n",
    "- At 75th percentile, the Size value is 1516. This means that 75% of the data points that lies below this 75th percentile point will have value equal to or less than 1516 sq ft. \n",
    "- For the high-level description purpose, let's create bins grouping certain sizes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [i*100 for i in range(int(property_data['Size'].quantile(0.75))//100)]\n",
    "bins.append(int(property_data['Size'].max()))\n",
    "labels = [\"{}-{}\".format(bins[i], bins[i+1]) for i in range(len(bins) - 1)]\n",
    "# print(bins)\n",
    "labels[-1]=\">{}\".format(bins[-2])\n",
    "# print(labels)\n",
    "\n",
    "property_data[\"Size_range\"] = pd.cut(property_data[\"Size\"], bins=bins, labels=labels)\n",
    "\n",
    "property_data.drop(['Size'], axis=1, inplace=True)   # Remove the column 'Size' because we are using 'Size_range' in place of that.\n",
    "# Display the first few rows of the DataFrame with the new column\n",
    "property_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de794d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Size_range':\\n\", property_data[\"Size_range\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Size_range\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab445355",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6158a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Posted_On':\\n\", property_data[\"Posted_On\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Posted_On\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34090",
   "metadata": {},
   "source": [
    "- Let's use AOI to categorize the values of 'Posted_On' dimension into the following values:\n",
    "    - `< an hour`:               This encompasses all the postings done within an hour\n",
    "    - `>= hour and < 1 day`:     This encompasses all the postings done between an hour (include) and a day (exclude)    \n",
    "    - `=> 1 day and < 1 month`:  This encompasses all the postings done between one day (include) and a month (exclude)    \n",
    "    - `=> 1 month and < 1 year`: This encompasses all the postings done between one month (include) and a year (exclude)    \n",
    "    - `=> 1 year and < 5 year`:  This encompasses all the postings done between one year (include) and five years (exclude)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.loc[property_data[\"Posted_On\"].str.contains('minute'), \"Posted_On_range\"] = '< an hour'\n",
    "property_data.loc[property_data[\"Posted_On\"].str.contains('hour'), \"Posted_On_range\"] = '=> hour and < 1 day'\n",
    "property_data.loc[property_data[\"Posted_On\"].str.contains('day'), \"Posted_On_range\"] = '=> 1 day and < 1 month'\n",
    "property_data.loc[property_data[\"Posted_On\"].str.contains('month'), \"Posted_On_range\"] = '=> 1 month and < 1 year'\n",
    "property_data.loc[property_data[\"Posted_On\"].str.contains('year'), \"Posted_On_range\"] = '=> 1 year and < 5 year'\n",
    "\n",
    "property_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02eaa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(property_data[\"Posted_On_range\"].value_counts())\n",
    "print(\".......................................................................\")\n",
    "print(\"Unique values\", property_data[\"Posted_On_range\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a965c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Posted_On'], axis=1, inplace=True)   # Remove the column 'Posted_On' because we are using 'Posted_On_range' in place of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b519b8b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Project_URL':\\n\", property_data[\"Project_URL\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Project_URL\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc30af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Project_URL'], axis=1, inplace=True)     # Dropping the dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ff7b7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Property_building_status':\\n\", property_data[\"Property_building_status\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Property_building_status\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b2bff",
   "metadata": {},
   "source": [
    "- We will use the values of 'Property_building_status' dimension as it is, because it is generalized into 3 distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb74de2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a206133",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'City_id':\\n\", property_data[\"City_id\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"City_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'City_name':\\n\", property_data[\"City_name\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"City_name\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f438ce",
   "metadata": {},
   "source": [
    "- As we can see, 'City_id' is just a unique ID corresponding to a unique 'City_name', that is both the dimensions represent the same entity.\n",
    "- Therefore, we can remove any one of them.\n",
    "- Let's remove 'City_id' as using 'City_name' is much more intuitive for humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f926853",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['City_id'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc4f85",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'No_of_BHK':\\n\", property_data[\"No_of_BHK\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"No_of_BHK\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07794e31",
   "metadata": {},
   "source": [
    "- We will use the values of 'No_of_BHK' dimension as it is because it is already characterized by 17 distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12fe739",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Locality_ID':\\n\", property_data[\"Locality_ID\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Locality_ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Locality_Name':\\n\", property_data[\"Locality_Name\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Locality_Name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4546e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Longitude':\\n\", property_data[\"Longitude\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Longitude\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Longitude'], axis=1, inplace=True)   # Remove the column 'Longitude'\n",
    "property_data.drop(['Latitude'], axis=1, inplace=True)   # Remove the column 'Latitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Sub_urban_ID':\\n\", property_data[\"Sub_urban_ID\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Sub_urban_ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Sub_urban_name':\\n\", property_data[\"Sub_urban_name\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Sub_urban_name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Sub_urban_ID'], axis=1, inplace=True)     # Remove the 'Sub_urban_ID' column as we already have 'Sub_urban_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf1b96",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8aa68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'description':\\n\", property_data[\"description\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"description\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['description'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc51741",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_furnished':\\n\", property_data[\"is_furnished\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_furnished\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45722d0",
   "metadata": {},
   "source": [
    "- Dimension 'is_furnished' has 3 distinct values and therefore we will use this as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'listing_domain_score':\\n\", property_data[\"listing_domain_score\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"listing_domain_score\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201d5c8",
   "metadata": {},
   "source": [
    "- Listing domain score is an important dimension that we want to keep. Also, it has 18 distinct values that we can use as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af232c18",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807abd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_plot':\\n\", property_data[\"is_plot\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_plot\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[property_data[\"is_plot\"] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394bd68",
   "metadata": {},
   "source": [
    "- Dimension 'is_plot' has 2 distinct values and it indicates that whether a property is a 'Residential Plot' or not.\n",
    "- We can see from above that 'is_plot'==True can also be represented by value 'Residential Plot' in the dimension 'Property_type'. Given this correlation, we can remove the dimension 'is_plot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['is_plot'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47190c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ffa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_RERA_registered':\\n\", property_data[\"is_RERA_registered\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_RERA_registered\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c46f5",
   "metadata": {},
   "source": [
    "- The dimension 'is_RERA_registered' has 2 distinct values and is already in a generalized form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfc79d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_Apartment':\\n\", property_data[\"is_Apartment\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_Apartment\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaf7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[property_data[\"is_Apartment\"] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83693a4",
   "metadata": {},
   "source": [
    "- Dimension 'is_Apartment' has 2 distinct values and it indicates that whether a property is an 'Apartment' or not.\n",
    "- We can see from above that 'is_Apartment'==True can also be represented by value 'Apartment' in the dimension 'Property_type'. Given this correlation, we can remove the dimension 'is_Apartment'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['is_Apartment'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591d1cf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac329bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_ready_to_move':\\n\", property_data[\"is_ready_to_move\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_ready_to_move\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[property_data[\"is_ready_to_move\"] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ccaa7",
   "metadata": {},
   "source": [
    "- Dimension 'is_ready_to_move' has 2 distinct values and it indicates that whether a property status is 'is_ready_to_move' or not.\n",
    "- We can see from above that 'is_ready_to_move'==True can also be represented by value 'Ready to move' in the dimension 'Property_status'. Given this correlation, we can remove the dimension 'is_ready_to_move'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086771c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['is_ready_to_move'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83542f7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_commercial_Listing':\\n\", property_data[\"is_commercial_Listing\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_commercial_Listing\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b55db1",
   "metadata": {},
   "source": [
    "- Dimension 'is_commercial_Listing' has only one value and therefore can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702395cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['is_commercial_Listing'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cd2c2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2083d6",
   "metadata": {},
   "source": [
    "- Now let us analyze the columns 'is_Pentahouse' and 'is_studio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_PentaHouse':\\n\", property_data[\"is_PentaHouse\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_PentaHouse\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68259968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'is_studio':\\n\", property_data[\"is_studio\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"is_studio\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be3d9d",
   "metadata": {},
   "source": [
    "- We know that both Pentahouse and Studio are a type of apartment.\n",
    "- Let us see how we can perform AOI, by incorporating value 'Pentahouse' and 'Studio' in the dimension 'Property_type' thus reducing the total number of dimensions without losing out on important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(property_data.loc[(property_data['is_PentaHouse']==True) & (property_data['is_studio']==True) & (property_data['Property_type'].str.startswith('Apartment')), ['Property_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9f278",
   "metadata": {},
   "source": [
    "- The above output shows that an apartment can never be both 'Pentahouse' and 'Studio' at the same time (which confirms to our intuitive understanding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36014c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(property_data.loc[(property_data['is_PentaHouse']==False) & (property_data['is_studio']==False) & (property_data['Property_type'].str.startswith('Apartment')), ['Property_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2b7d6",
   "metadata": {},
   "source": [
    "- The above output shows that an apartment need not be either a 'Pentahouse' or 'Studio'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8914cb7",
   "metadata": {},
   "source": [
    "- Equipped with this knowledge, let's incorporate two more values in the dimension 'Property_type', i.e., `Pentahouse_Apartment` and `Studio_Apartment`.\n",
    "- This will lead to overall 7 values in the dimension 'Property_type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea49160",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.loc[property_data[\"is_PentaHouse\"]==True, \"Property_type\"] = 'Pentahouse_Apartment'\n",
    "property_data.loc[property_data[\"is_studio\"]==True, \"Property_type\"] = 'Studio_Apartment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e99de",
   "metadata": {},
   "source": [
    "- Let's see the value of dimension 'Property_type' if 'is_studio'==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecdd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[property_data[\"is_studio\"] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b394c",
   "metadata": {},
   "source": [
    "- Similarly, let's see the value of dimension 'Property_type' if 'is_PentaHouse'==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[property_data[\"is_PentaHouse\"] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b1729",
   "metadata": {},
   "source": [
    "- Let's verify the values of the dimension 'Property_type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ce69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Property_type':\\n\", property_data[\"Property_type\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Property_type\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ed655",
   "metadata": {},
   "source": [
    "- Let's drop the dimensions 'is_PentaHouse' and 'is_studio', as they have been successfully incorporated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['is_PentaHouse', 'is_studio'], axis=1, inplace=True)     # Remove the mentioned two columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62092891",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197443b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in coloumn 'Listing_Category':\\n\", property_data[\"Listing_Category\"].unique())\n",
    "print(\".......................................................................\")\n",
    "print(\"Number of unique values:\", property_data[\"Listing_Category\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca968047",
   "metadata": {},
   "source": [
    "- Dimension 'Listing_Category' has only one value and therefore can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93089a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.drop(['Listing_Category'], axis=1, inplace=True)     # Remove the mentioned column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e65ba",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the dataset:', property_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c40c9",
   "metadata": {},
   "source": [
    "- As we can observe, after performing AOI, our dimension size has come down from 32 to 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdfb48",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b56b9",
   "metadata": {},
   "source": [
    "- Final dataframe looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacb641",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1141bd",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
